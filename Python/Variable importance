''' Variable importance '''

from sklearn.ensemble import RandomForestRegressor

dataset = pd.read_csv('.../Clean Data_Random Forest_3.csv')

# When response variable is annual costs,

forest = RandomForestRegressor(n_estimators = 100, min_samples_split = 10, min_samples_leaf= 3, max_features = 'auto', max_depth= 10, bootstrap=True) 
forest.fit(X_train, y_train) 

# When response variable is active ingredients,

forest = RandomForestRegressor(n_estimators = 20, min_samples_split = 6, min_samples_leaf= 4, max_features = 'auto', max_depth= 30, bootstrap=True) 
forest.fit(X_train, y_train) 

# Determine feature importance values

import numpy as np 
importances = forest.feature_importances_


# Obtaining feature names
forest.feature_names_in_

# Creating importances_df dataframe
importances_df = pd.DataFrame({"feature_names" : forest.feature_names_in_, 
                               "importances" : forest.feature_importances_})

# Plotting bar chart, g is from graph
#  x="importances",  y="feature_names"

g = sns.barplot(x=importances_df["feature_names"], 
                y=importances_df["importances"])
g.set_title("Feature importances", fontsize=14);      

g = sns.barplot(data=importances_df, 
                x="importances", 
                y="feature_names")
g.set_title("Feature importances", fontsize=14)
for value in g.containers:
    g.bar_label(value)


# Sort the feature importance in descending order

sorted_indices = np.argsort(importances)[::-1] 
feat_labels = dataset.columns[1:] 
for f in range(X_train.shape[1]):
    print("%2d) %-*s %f" % (f + 1, 30,
                            feat_labels[sorted_indices[f]],
                            importances[sorted_indices[f]]))

# Visualize the feature importance

import matplotlib.pyplot as plt
 
plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]), importances[sorted_indices], align='center')
plt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)
plt.tight_layout()
plt.show()
